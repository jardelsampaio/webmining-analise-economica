{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277bc573",
   "metadata": {},
   "source": [
    "# Coleta de Notícias — Fintech de Investimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5edac",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install pandas requests beautifulsoup4 lxml html5lib duckdb tqdm\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f3fdb",
   "metadata": {},
   "source": [
    "## Importações e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f21e1c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ambiente pronto.\n"
     ]
    }
   ],
   "source": [
    "import os, time, re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                   \"(KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36 OPR/104.0.0.0\"),\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\"\n",
    "}\n",
    "\n",
    "def fetch(url, params=None, sleep=1.0):\n",
    "    r = requests.get(url, headers=HEADERS, params=params, timeout=30)\n",
    "    time.sleep(sleep)\n",
    "    r.raise_for_status()\n",
    "    return r\n",
    "print('Ambiente pronto.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2623e",
   "metadata": {},
   "source": [
    "## Funções Utilitárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d41f0218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt: str) -> str:\n",
    "    if not txt: return ''\n",
    "    return re.sub(r'\\s+', ' ', str(txt)).strip()\n",
    "\n",
    "def to_iso_from_any(s: str):\n",
    "    if not s: return None\n",
    "    s = str(s)\n",
    "\n",
    "    try:\n",
    "        from email.utils import parsedate_to_datetime\n",
    "        return parsedate_to_datetime(s).date().isoformat()\n",
    "    except Exception:\n",
    "        pass\n",
    "    # tenta yyyy-mm-dd\n",
    "    m = re.search(r'(\\d{4}-\\d{2}-\\d{2})', s)\n",
    "    if m: return m.group(1)\n",
    "    # tenta dd/mm/yyyy\n",
    "    try:\n",
    "        from datetime import datetime\n",
    "        return datetime.strptime(s, '%d/%m/%Y').date().isoformat()\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85175786",
   "metadata": {},
   "source": [
    "## InfoMoney — HTML (últimas notícias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56e5fe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InfoMoney (HTML) itens: 20\n"
     ]
    }
   ],
   "source": [
    "def coleta_infomoney_html(pages=2):\n",
    "    base = \"https://www.infomoney.com.br/ultimas-noticias/\"\n",
    "    itens = []\n",
    "    for p in range(1, pages+1):\n",
    "        url = base if p == 1 else f\"{base}page/{p}/\"\n",
    "        try:\n",
    "            resp = fetch(url)\n",
    "            soup = BeautifulSoup(resp.text, 'lxml')\n",
    "        except Exception as e:\n",
    "            print(f'Falha ao acessar {url}:', e)\n",
    "            continue\n",
    "        for art in soup.select('article, div.card, li'):\n",
    "            a = art.find('a', href=True)\n",
    "            if not a: continue\n",
    "            titulo = clean_text(a.get_text())\n",
    "            link = urljoin(base, a['href'])\n",
    "            if not link.startswith('https://www.infomoney.com.br'): continue\n",
    "            if len(titulo) < 20: continue\n",
    "            ttag = art.find('time')\n",
    "            dt = (ttag.get('datetime') or ttag.get_text(strip=True)) if ttag else None\n",
    "            itens.append({'fonte':'InfoMoney','titulo':titulo,'url':link,'data_publicacao_raw':dt})\n",
    "    return pd.DataFrame(itens)\n",
    "\n",
    "df_im = coleta_infomoney_html(pages=2)\n",
    "print('InfoMoney (HTML) itens:', len(df_im))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af321fd8",
   "metadata": {},
   "source": [
    "## Coleta via RSS — SeuDinheiro, InfoMoney, Exame Invest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fcefc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coleta_rss(url_feed: str, fonte: str, max_itens=80):\n",
    "    try:\n",
    "        resp = fetch(url_feed)\n",
    "        soup = BeautifulSoup(resp.content, 'xml')  # parser XML\n",
    "    except Exception as e:\n",
    "        print(f'Falha no RSS {fonte}:', e)\n",
    "        return pd.DataFrame()\n",
    "    itens = []\n",
    "    for item in soup.find_all(['item','entry'])[:max_itens]:\n",
    "        titulo = clean_text(item.title.get_text() if item.title else None)\n",
    "        link = None\n",
    "        if item.link:\n",
    "            link = item.link.get('href') or item.link.get_text()\n",
    "        elif item.find('guid'):\n",
    "            link = item.find('guid').get_text()\n",
    "        pub = None\n",
    "        for tag in ['pubDate','published','updated','dc:date']:\n",
    "            t = item.find(tag)\n",
    "            if t:\n",
    "                pub = clean_text(t.get_text())\n",
    "                break\n",
    "        if not titulo or not link: continue\n",
    "        itens.append({'fonte':fonte,'titulo':titulo,'url':link,'data_publicacao_raw':pub})\n",
    "    return pd.DataFrame(itens)\n",
    "\n",
    "rss_sd   = 'https://www.seudinheiro.com/feed/'\n",
    "rss_im   = 'https://www.infomoney.com.br/feed/'\n",
    "rss_ex   = 'https://exame.com/feed/'\n",
    "\n",
    "df_sd = coleta_rss(rss_sd, 'SeuDinheiro')\n",
    "df_im_rss = coleta_rss(rss_im, 'InfoMoney')\n",
    "df_ex = coleta_rss(rss_ex, 'ExameInvest')\n",
    "\n",
    "print('SeuDinheiro (RSS) itens:', len(df_sd))\n",
    "print('InfoMoney (RSS) itens:', len(df_im_rss))\n",
    "print('Exame Invest (RSS) itens:', len(df_ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0333e",
   "metadata": {},
   "source": [
    "## Consolidação, Deduplicação e Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "df_all = pd.concat([df_im, df_im_rss, df_sd, df_ex], ignore_index=True)\n",
    "df_all['titulo_norm'] = df_all['titulo'].str.lower().str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "df_all = df_all.drop_duplicates(subset=['url']).drop_duplicates(subset=['titulo_norm']).drop(columns=['titulo_norm'])\n",
    "df_all['data_publicacao'] = df_all['data_publicacao_raw'].apply(to_iso_from_any)\n",
    "df_all['coletado_em'] = datetime.now(timezone.utc).isoformat(timespec='seconds')\n",
    "df_all = df_all[['fonte','titulo','url','data_publicacao','coletado_em']]\n",
    "print('Total após limpeza:', len(df_all))\n",
    "df_all.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402cc1ab",
   "metadata": {},
   "source": [
    "## Salvando Dataset cru (raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cfd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "hoje = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "csv_path = f'data/raw/noticias_{hoje}.csv'\n",
    "jsonl_path = f'data/raw/noticias_{hoje}.jsonl'\n",
    "\n",
    "df_all.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "with open(jsonl_path, 'w', encoding='utf-8') as f:\n",
    "    for _, row in df_all.iterrows():\n",
    "        f.write(json.dumps(row.to_dict(), ensure_ascii=False) + '\\n')\n",
    "\n",
    "print('Arquivos salvos:')\n",
    "print(' -', csv_path)\n",
    "print(' -', jsonl_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7e7633",
   "metadata": {},
   "source": [
    "## Inserção no DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ad0e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total em noticias: 55\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "conn = duckdb.connect('banco/banco_analitico.duckdb')\n",
    "conn.execute('''\n",
    "CREATE TABLE IF NOT EXISTS noticias (\n",
    "    fonte           VARCHAR,\n",
    "    titulo          VARCHAR,\n",
    "    url             VARCHAR,\n",
    "    data_publicacao DATE,\n",
    "    coletado_em     TIMESTAMP\n",
    ")\n",
    "''')\n",
    "conn.register('tmp_df', df_all)\n",
    "conn.execute(\"\"\"\n",
    "INSERT INTO noticias\n",
    "SELECT fonte, titulo, url,\n",
    "       CASE WHEN data_publicacao IS NULL OR data_publicacao = '' THEN NULL\n",
    "            ELSE CAST(data_publicacao AS DATE) END,\n",
    "       CAST(coletado_em AS TIMESTAMPTZ)\n",
    "FROM tmp_df\n",
    "\"\"\")\n",
    "print('Total em noticias:', conn.execute('SELECT COUNT(*) FROM noticias').fetchone()[0])\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e8ba1-04dc-40e3-be49-a5f77786f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect('banco/banco_analitico.duckdb')\n",
    "df = conn.execute(\"SELECT * FROM noticias\").fetchdf()\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
